{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numpy experimentation\n",
    "a=np.ones((10))\n",
    "b=np.zeros(10)\n",
    "b[1]=1\n",
    "print a.shape, \" \", b.shape\n",
    "c =1.2*np.outer(a,b)\n",
    "print (a*b)\n",
    "print np.square(c)\n",
    "print c\n",
    "print c[:,1].shape\n",
    "a[0][b==0]=4\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import sys\n",
    "import gzip\n",
    "\n",
    "def read_file(filename):\n",
    "    with gzip.open(filename,'rb') as fp:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', fp.read(4))\n",
    "        shape = tuple(struct.unpack('>I', fp.read(4))[0] for d in range(dims))\n",
    "        np_array = np.frombuffer(fp.read(), dtype=np.uint8).reshape(shape)\n",
    "    return np_array\n",
    "\n",
    "def preprocess_type1(image_file, label_file):\n",
    "    images = read_file(image_file)\n",
    "    labels = read_file(label_file)\n",
    "    if (len(labels) > 10000):\n",
    "        labels = labels[:10000]\n",
    "        images = images[:10000]    \n",
    "    images = images/255.0\n",
    "    images = images.reshape( (10000, 784))\n",
    "\n",
    "    labels = labels.reshape(-1,1)\n",
    "    data = np.concatenate((images, labels), axis=1)\n",
    "    np.random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "def preprocess_type2(image_file, label_file):\n",
    "    images = read_file(image_file)\n",
    "    labels = read_file(label_file)\n",
    "    if (len(labels) > 10000):\n",
    "        labels = labels[:10000]\n",
    "        images = images[:10000]    \n",
    "    images = images/255.0\n",
    "    images_sample = np.zeros((10000, 14, 14))\n",
    "    for i,image in enumerate(images):\n",
    "        for j in range(14):\n",
    "            for k in range(14):\n",
    "                images_sample[i][j][k] = max(image[j*2][k*2], image[j*2][k*2+1],\n",
    "                                                 image[j*2+1][k*2], image[j*2+1][k*2+1])\n",
    "    images_sample = images_sample.reshape( (10000, 196))\n",
    "\n",
    "    labels = labels.reshape(-1,1)\n",
    "    data = np.concatenate((images_sample, labels), axis=1)\n",
    "    np.random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "def get_features_labels(data, bias):\n",
    "    examples = data[:,:-1]\n",
    "    labels = data[:,-1]\n",
    "    classifier_labels = np.zeros((10, len(labels)))\n",
    "    for i in range(10):\n",
    "        classifier_labels[i][labels == i] = 1\n",
    "    examples = np.append(examples, bias, 1)\n",
    "    return examples, labels, classifier_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(test_data, weights):\n",
    "    data_size = len(test_data)\n",
    "    bias = np.ones((data_size,1))\n",
    "    examples, labels, classifier_labels = get_features_labels(test_data, bias)\n",
    "    prediction = np.ones(data_size, dtype = int)\n",
    "    correct = 0\n",
    "    for i, example in enumerate(examples):\n",
    "        activation_values = sigmoid(np.sum(weights*example, axis = 1))\n",
    "        prediction[i] = np.argmax(activation_values)\n",
    "        if prediction[i] == labels[i]:\n",
    "            correct += 1\n",
    "    accuracy = correct*1.0/data_size\n",
    "    return prediction, labels, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "def sigmoid(value):\n",
    "    value[value > 100] = 100\n",
    "    value[value < -100] = -100\n",
    "    return 1/(1+np.exp(-value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hw2/DATA_FOLDER/\"\n",
    "train_data_path = path + '/train-images-idx3-ubyte.gz'\n",
    "train_labels_path = path + '/train-labels-idx1-ubyte.gz'\n",
    "test_data_path = path + '/t10k-images-idx3-ubyte.gz'\n",
    "test_labels_path = path + '/t10k-labels-idx1-ubyte.gz'\n",
    "train_data_type1 = preprocess_type1(path + '/train-images-idx3-ubyte.gz', path + '/train-labels-idx1-ubyte.gz')\n",
    "# print train_data[1]\n",
    "test_data_type1 = preprocess_type1(path + '/t10k-images-idx3-ubyte.gz', path + '/t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "train_data_type2 = preprocess_type2(path + '/train-images-idx3-ubyte.gz', path + '/train-labels-idx1-ubyte.gz')\n",
    "# print train_data[1]\n",
    "test_data_type2 = preprocess_type2(path + '/t10k-images-idx3-ubyte.gz', path + '/t10k-labels-idx1-ubyte.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_gd(train_data, num_epoches, learning_rate, lamda, test_data):\n",
    "    data_size = len(train_data)\n",
    "    feature_length = train_data.shape[1]\n",
    "    weights = np.random.uniform(-0.1,0.1,[10,feature_length])\n",
    "    bias = np.ones((data_size,1))\n",
    "    prev_loss = 1000\n",
    "    for epoch in range(num_epoches):\n",
    "        loss = np.zeros(10)\n",
    "        np.random.shuffle(train_data)\n",
    "        examples, labels, classifier_labels = get_features_labels(train_data, bias)\n",
    "        delta_weights = np.zeros((10,feature_length))\n",
    "        for i,example in enumerate(examples):\n",
    "            z = np.sum(weights*example, axis = 1)\n",
    "            y_pred = sigmoid(z)\n",
    "            delta_weights += learning_rate*np.outer(y_pred-classifier_labels[:,i],example)\n",
    "            loss+=classifier_labels[:,i]*np.log(y_pred+0.0000000001)+(1-classifier_labels[:,i])*np.log(1-y_pred+0.0000000001)\n",
    "        \n",
    "        weights = weights - delta_weights - learning_rate*lamda*weights\n",
    "        loss = loss*-1.0/data_size + lamda*np.sum(np.square(weights), axis = 1)/2.0\n",
    "        loss = np.sum(loss)/10.0\n",
    "        train_prediction, train_labels, train_accuracy = inference(train_data[:train_data_size], weights)\n",
    "        test_prediction, test_labels, test_accuracy = inference(test_data, weights)\n",
    "        print \"epoch \", epoch, \": Training loss: \", loss, \" Training Accuracy: \", train_accuracy, \" Test Accuracy: \", test_accuracy\n",
    "        delta_loss = prev_loss - loss\n",
    "        prev_loss = loss\n",
    "        if delta_loss > 0 and delta_loss < 0.0001 and epoch > 100:\n",
    "            return weights, loss\n",
    "    return weights, loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_sgd(train_data, num_epoches, learning_rate, lamda, test_data):\n",
    "    data_size = len(train_data)\n",
    "    feature_length = train_data.shape[1]\n",
    "    weights = np.random.uniform(-0.1,0.1,[10,feature_length])\n",
    "    bias = np.ones((data_size,1))\n",
    "    prev_loss = 1000\n",
    "    batch_size_loss = 1000\n",
    "    for epoch in range(num_epoches):\n",
    "        loss = np.zeros(10)\n",
    "        np.random.shuffle(train_data)\n",
    "        examples, labels, classifier_labels = get_features_labels(train_data, bias)\n",
    "        for i,example in enumerate(examples):\n",
    "            delta_weights = np.zeros((10,feature_length))\n",
    "            z = np.sum(weights*example, axis = 1)\n",
    "            y_pred = sigmoid(z)\n",
    "            delta_weights += learning_rate*np.outer(y_pred-classifier_labels[:,i],example)\n",
    "            weights = weights - delta_weights - learning_rate*lamda*weights\n",
    "            loss+=classifier_labels[:,i]*np.log(y_pred+0.0000000001)+(1-classifier_labels[:,i])*np.log(1-y_pred+0.0000000001)\n",
    "\n",
    "            if i%batch_size_loss==0:\n",
    "                loss = loss*-1.0/batch_size_loss + lamda*np.sum(np.square(weights), axis = 1)/2.0\n",
    "                loss = np.sum(loss)/10.0\n",
    "                \n",
    "                train_prediction, train_labels, train_accuracy = inference(train_data[:train_data_size], weights)\n",
    "                test_prediction, test_labels, test_accuracy = inference(test_data, weights)\n",
    "                print \"epoch \", epoch, \": Training loss: \", loss, \" Training Accuracy: \", train_accuracy, \" Test Accuracy: \", test_accuracy\n",
    "                delta_loss = prev_loss - loss\n",
    "                prev_loss = loss\n",
    "                loss = np.zeros(10)\n",
    "    #             print \"delta loss \", delta_loss\n",
    "                if delta_loss < 0.00007 and train_accuracy > 0.92:\n",
    "                    return weights, loss\n",
    "    return weights, loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stochastic gradient descent\n",
    "train_data_size = 10000\n",
    "epochs = 20\n",
    "learning_rate = 0.01\n",
    "lamda = 0.000001\n",
    "# lamda = 0\n",
    "gd_weights, train_loss = train_sgd(train_data_type1[:10000], epochs, learning_rate, lamda, test_data_type1)\n",
    "test_prediction, test_labels, test_accuracy = inference(test_data_type1, gd_weights)\n",
    "print \"test accuracy \", test_accuracy, \" training loss: \", train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_weights, train_loss = train_sgd(train_data_type2[:10000], epochs, learning_rate, lamda, test_data_type2)\n",
    "test_prediction, test_labels, test_accuracy = inference(test_data_type2, gd_weights)\n",
    "print \"test accuracy \", test_accuracy, \" training loss: \", train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 : Training loss:  0.749490814214  Training Accuracy:  0.1001  Test Accuracy:  0.098\n",
      "epoch  1 : Training loss:  1.35691121953  Training Accuracy:  0.108  Test Accuracy:  0.1034\n",
      "epoch  2 : Training loss:  0.89492671952  Training Accuracy:  0.2397  Test Accuracy:  0.2363\n",
      "epoch  3 : Training loss:  0.499278318433  Training Accuracy:  0.5862  Test Accuracy:  0.5779\n",
      "epoch  4 : Training loss:  0.225907814483  Training Accuracy:  0.7549  Test Accuracy:  0.7394\n",
      "epoch  5 : Training loss:  0.169404270122  Training Accuracy:  0.7589  Test Accuracy:  0.7518\n",
      "epoch  6 : Training loss:  0.157214788406  Training Accuracy:  0.7384  Test Accuracy:  0.7328\n",
      "epoch  7 : Training loss:  0.182833599461  Training Accuracy:  0.7941  Test Accuracy:  0.7835\n",
      "epoch  8 : Training loss:  0.139557626532  Training Accuracy:  0.7977  Test Accuracy:  0.7882\n",
      "epoch  9 : Training loss:  0.139282669497  Training Accuracy:  0.7694  Test Accuracy:  0.7562\n",
      "epoch  10 : Training loss:  0.14154441417  Training Accuracy:  0.8012  Test Accuracy:  0.7908\n",
      "epoch  11 : Training loss:  0.135027981091  Training Accuracy:  0.7596  Test Accuracy:  0.7495\n",
      "epoch  12 : Training loss:  0.148690354416  Training Accuracy:  0.8228  Test Accuracy:  0.8174\n",
      "epoch  13 : Training loss:  0.123565437581  Training Accuracy:  0.8125  Test Accuracy:  0.8029\n",
      "epoch  14 : Training loss:  0.12711878152  Training Accuracy:  0.8252  Test Accuracy:  0.8195\n",
      "epoch  15 : Training loss:  0.121613101607  Training Accuracy:  0.8092  Test Accuracy:  0.802\n",
      "epoch  16 : Training loss:  0.129668759977  Training Accuracy:  0.8351  Test Accuracy:  0.83\n",
      "epoch  17 : Training loss:  0.116846150979  Training Accuracy:  0.828  Test Accuracy:  0.8192\n",
      "epoch  18 : Training loss:  0.122487223371  Training Accuracy:  0.841  Test Accuracy:  0.8378\n",
      "epoch  19 : Training loss:  0.11355316849  Training Accuracy:  0.8409  Test Accuracy:  0.8303\n",
      "epoch  20 : Training loss:  0.118252332437  Training Accuracy:  0.8486  Test Accuracy:  0.8438\n",
      "epoch  21 : Training loss:  0.110316035364  Training Accuracy:  0.8542  Test Accuracy:  0.842\n",
      "epoch  22 : Training loss:  0.11372024106  Training Accuracy:  0.8564  Test Accuracy:  0.8497\n",
      "epoch  23 : Training loss:  0.10748462388  Training Accuracy:  0.8609  Test Accuracy:  0.8512\n",
      "epoch  24 : Training loss:  0.109880421655  Training Accuracy:  0.8627  Test Accuracy:  0.8564\n",
      "epoch  25 : Training loss:  0.104943949479  Training Accuracy:  0.868  Test Accuracy:  0.8589\n",
      "epoch  26 : Training loss:  0.10652984856  Training Accuracy:  0.8665  Test Accuracy:  0.8605\n",
      "epoch  27 : Training loss:  0.102657386022  Training Accuracy:  0.8717  Test Accuracy:  0.8628\n",
      "epoch  28 : Training loss:  0.10361883373  Training Accuracy:  0.8711  Test Accuracy:  0.8635\n",
      "epoch  29 : Training loss:  0.100595263782  Training Accuracy:  0.8746  Test Accuracy:  0.867\n",
      "epoch  30 : Training loss:  0.101094206266  Training Accuracy:  0.8749  Test Accuracy:  0.8665\n",
      "epoch  31 : Training loss:  0.0987353988075  Training Accuracy:  0.8781  Test Accuracy:  0.8709\n",
      "epoch  32 : Training loss:  0.0989061593045  Training Accuracy:  0.8778  Test Accuracy:  0.8702\n",
      "epoch  33 : Training loss:  0.0970609244703  Training Accuracy:  0.8807  Test Accuracy:  0.8727\n",
      "epoch  34 : Training loss:  0.0970096876709  Training Accuracy:  0.8807  Test Accuracy:  0.8721\n",
      "epoch  35 : Training loss:  0.0955572125428  Training Accuracy:  0.8833  Test Accuracy:  0.8765\n",
      "epoch  36 : Training loss:  0.0953639889101  Training Accuracy:  0.8831  Test Accuracy:  0.8744\n",
      "epoch  37 : Training loss:  0.0942095218719  Training Accuracy:  0.8853  Test Accuracy:  0.8779\n",
      "epoch  38 : Training loss:  0.0939316849579  Training Accuracy:  0.8848  Test Accuracy:  0.8769\n",
      "epoch  39 : Training loss:  0.0930020098594  Training Accuracy:  0.8857  Test Accuracy:  0.8792\n",
      "epoch  40 : Training loss:  0.0926786257093  Training Accuracy:  0.8857  Test Accuracy:  0.8775\n",
      "epoch  41 : Training loss:  0.0919179018181  Training Accuracy:  0.887  Test Accuracy:  0.8813\n",
      "epoch  42 : Training loss:  0.0915741969987  Training Accuracy:  0.8881  Test Accuracy:  0.8799\n",
      "epoch  43 : Training loss:  0.0909403289414  Training Accuracy:  0.8881  Test Accuracy:  0.8817\n",
      "epoch  44 : Training loss:  0.0905917778807  Training Accuracy:  0.8892  Test Accuracy:  0.8812\n",
      "epoch  45 : Training loss:  0.0900533535111  Training Accuracy:  0.8906  Test Accuracy:  0.8818\n",
      "epoch  46 : Training loss:  0.0897090268117  Training Accuracy:  0.8898  Test Accuracy:  0.8824\n",
      "epoch  47 : Training loss:  0.0892428066211  Training Accuracy:  0.8914  Test Accuracy:  0.8839\n",
      "epoch  48 : Training loss:  0.0889078235081  Training Accuracy:  0.8904  Test Accuracy:  0.8836\n",
      "epoch  49 : Training loss:  0.088496728762  Training Accuracy:  0.8923  Test Accuracy:  0.8848\n",
      "epoch  50 : Training loss:  0.088173861912  Training Accuracy:  0.8915  Test Accuracy:  0.8849\n",
      "epoch  51 : Training loss:  0.087805392036  Training Accuracy:  0.8932  Test Accuracy:  0.8868\n",
      "epoch  52 : Training loss:  0.0874960153017  Training Accuracy:  0.8926  Test Accuracy:  0.8862\n",
      "epoch  53 : Training loss:  0.0871610255027  Training Accuracy:  0.8942  Test Accuracy:  0.8881\n",
      "epoch  54 : Training loss:  0.0868656377874  Training Accuracy:  0.8934  Test Accuracy:  0.8873\n",
      "epoch  55 : Training loss:  0.086557414722  Training Accuracy:  0.8944  Test Accuracy:  0.8886\n",
      "epoch  56 : Training loss:  0.0862759347045  Training Accuracy:  0.8945  Test Accuracy:  0.8881\n",
      "epoch  57 : Training loss:  0.0859895137598  Training Accuracy:  0.8952  Test Accuracy:  0.8889\n",
      "epoch  58 : Training loss:  0.0857214684952  Training Accuracy:  0.8951  Test Accuracy:  0.8886\n",
      "epoch  59 : Training loss:  0.0854531404945  Training Accuracy:  0.8956  Test Accuracy:  0.8899\n",
      "epoch  60 : Training loss:  0.0851978069564  Training Accuracy:  0.8955  Test Accuracy:  0.8895\n",
      "epoch  61 : Training loss:  0.0849447673551  Training Accuracy:  0.8959  Test Accuracy:  0.8909\n",
      "epoch  62 : Training loss:  0.0847012865795  Training Accuracy:  0.8963  Test Accuracy:  0.8906\n",
      "epoch  63 : Training loss:  0.0844613876888  Training Accuracy:  0.8964  Test Accuracy:  0.8916\n",
      "epoch  64 : Training loss:  0.0842288543188  Training Accuracy:  0.8966  Test Accuracy:  0.8916\n",
      "epoch  65 : Training loss:  0.0840004298691  Training Accuracy:  0.8969  Test Accuracy:  0.892\n",
      "epoch  66 : Training loss:  0.0837779565121  Training Accuracy:  0.8968  Test Accuracy:  0.8921\n",
      "epoch  67 : Training loss:  0.0835596962842  Training Accuracy:  0.8972  Test Accuracy:  0.8921\n",
      "epoch  68 : Training loss:  0.083346454022  Training Accuracy:  0.8973  Test Accuracy:  0.8919\n",
      "epoch  69 : Training loss:  0.0831373131588  Training Accuracy:  0.8976  Test Accuracy:  0.8922\n",
      "epoch  70 : Training loss:  0.0829325521036  Training Accuracy:  0.8981  Test Accuracy:  0.8923\n",
      "epoch  71 : Training loss:  0.0827316847423  Training Accuracy:  0.8981  Test Accuracy:  0.8922\n",
      "epoch  72 : Training loss:  0.0825347398951  Training Accuracy:  0.8986  Test Accuracy:  0.8929\n",
      "epoch  73 : Training loss:  0.0823414501744  Training Accuracy:  0.8986  Test Accuracy:  0.8931\n",
      "epoch  74 : Training loss:  0.0821517378649  Training Accuracy:  0.899  Test Accuracy:  0.8938\n",
      "epoch  75 : Training loss:  0.0819654435859  Training Accuracy:  0.8993  Test Accuracy:  0.8944\n",
      "epoch  76 : Training loss:  0.0817824529006  Training Accuracy:  0.8995  Test Accuracy:  0.8944\n",
      "epoch  77 : Training loss:  0.0816026585509  Training Accuracy:  0.8998  Test Accuracy:  0.8945\n",
      "epoch  78 : Training loss:  0.0814259409312  Training Accuracy:  0.9001  Test Accuracy:  0.8944\n",
      "epoch  79 : Training loss:  0.081252217707  Training Accuracy:  0.9005  Test Accuracy:  0.8946\n",
      "epoch  80 : Training loss:  0.0810813767027  Training Accuracy:  0.9005  Test Accuracy:  0.8947\n",
      "epoch  81 : Training loss:  0.0809133477745  Training Accuracy:  0.9005  Test Accuracy:  0.8953\n",
      "epoch  82 : Training loss:  0.0807480299843  Training Accuracy:  0.901  Test Accuracy:  0.8955\n",
      "epoch  83 : Training loss:  0.0805853596763  Training Accuracy:  0.9013  Test Accuracy:  0.8957\n",
      "epoch  84 : Training loss:  0.0804252472451  Training Accuracy:  0.9014  Test Accuracy:  0.8957\n",
      "epoch  85 : Training loss:  0.0802676331066  Training Accuracy:  0.9014  Test Accuracy:  0.8959\n",
      "epoch  86 : Training loss:  0.0801124377602  Training Accuracy:  0.9015  Test Accuracy:  0.8957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  87 : Training loss:  0.0799596047451  Training Accuracy:  0.9018  Test Accuracy:  0.8959\n",
      "epoch  88 : Training loss:  0.0798090631522  Training Accuracy:  0.902  Test Accuracy:  0.896\n",
      "epoch  89 : Training loss:  0.0796607593116  Training Accuracy:  0.9024  Test Accuracy:  0.8962\n",
      "epoch  90 : Training loss:  0.0795146295054  Training Accuracy:  0.9027  Test Accuracy:  0.8964\n",
      "epoch  91 : Training loss:  0.0793706227519  Training Accuracy:  0.9028  Test Accuracy:  0.8966\n",
      "epoch  92 : Training loss:  0.0792286813527  Training Accuracy:  0.9028  Test Accuracy:  0.8967\n",
      "epoch  93 : Training loss:  0.0790887569769  Training Accuracy:  0.903  Test Accuracy:  0.8968\n",
      "epoch  94 : Training loss:  0.0789507970044  Training Accuracy:  0.9034  Test Accuracy:  0.8968\n",
      "epoch  95 : Training loss:  0.0788147557166  Training Accuracy:  0.9037  Test Accuracy:  0.8971\n",
      "epoch  96 : Training loss:  0.0786805848266  Training Accuracy:  0.9037  Test Accuracy:  0.8972\n",
      "epoch  97 : Training loss:  0.0785482411684  Training Accuracy:  0.9037  Test Accuracy:  0.8974\n",
      "epoch  98 : Training loss:  0.0784176801981  Training Accuracy:  0.904  Test Accuracy:  0.8975\n",
      "epoch  99 : Training loss:  0.0782888612179  Training Accuracy:  0.9044  Test Accuracy:  0.8977\n",
      "epoch  100 : Training loss:  0.0781617429567  Training Accuracy:  0.9043  Test Accuracy:  0.8979\n",
      "epoch  101 : Training loss:  0.0780362870797  Training Accuracy:  0.9044  Test Accuracy:  0.8981\n",
      "epoch  102 : Training loss:  0.0779124552091  Training Accuracy:  0.9045  Test Accuracy:  0.8982\n",
      "epoch  103 : Training loss:  0.0777902112548  Training Accuracy:  0.9048  Test Accuracy:  0.8983\n",
      "epoch  104 : Training loss:  0.0776695194211  Training Accuracy:  0.9051  Test Accuracy:  0.8984\n",
      "epoch  105 : Training loss:  0.0775503457362  Training Accuracy:  0.9051  Test Accuracy:  0.8986\n",
      "epoch  106 : Training loss:  0.0774326567279  Training Accuracy:  0.9056  Test Accuracy:  0.8985\n",
      "epoch  107 : Training loss:  0.0773164204144  Training Accuracy:  0.906  Test Accuracy:  0.8987\n",
      "epoch  108 : Training loss:  0.0772016054286  Training Accuracy:  0.906  Test Accuracy:  0.899\n",
      "epoch  109 : Training loss:  0.077088181651  Training Accuracy:  0.9063  Test Accuracy:  0.8993\n",
      "epoch  110 : Training loss:  0.0769761196328  Training Accuracy:  0.9065  Test Accuracy:  0.8993\n",
      "epoch  111 : Training loss:  0.0768653909934  Training Accuracy:  0.9065  Test Accuracy:  0.8993\n",
      "epoch  112 : Training loss:  0.0767559680403  Training Accuracy:  0.9066  Test Accuracy:  0.8997\n",
      "epoch  113 : Training loss:  0.0766478240138  Training Accuracy:  0.9068  Test Accuracy:  0.8997\n",
      "epoch  114 : Training loss:  0.0765409328361  Training Accuracy:  0.907  Test Accuracy:  0.8997\n",
      "epoch  115 : Training loss:  0.0764352692569  Training Accuracy:  0.9072  Test Accuracy:  0.8998\n",
      "epoch  116 : Training loss:  0.0763308086873  Training Accuracy:  0.9073  Test Accuracy:  0.9\n",
      "epoch  117 : Training loss:  0.0762275272827  Training Accuracy:  0.9076  Test Accuracy:  0.9001\n",
      "epoch  118 : Training loss:  0.0761254018306  Training Accuracy:  0.9078  Test Accuracy:  0.9003\n",
      "epoch  119 : Training loss:  0.0760244097953  Training Accuracy:  0.9082  Test Accuracy:  0.9003\n",
      "epoch  120 : Training loss:  0.0759245292403  Training Accuracy:  0.9084  Test Accuracy:  0.9004\n",
      "test accuracy  0.9004  training loss:  0.0759245292403\n"
     ]
    }
   ],
   "source": [
    "#batch gradient descent\n",
    "\n",
    "train_data_size = 10000\n",
    "epochs = 300\n",
    "learning_rate = 0.0001\n",
    "lamda = 0.0001\n",
    "# lamda = 0\n",
    "gd_weights, train_loss = train_gd(train_data_type1[:10000], epochs, learning_rate, lamda, test_data_type1)\n",
    "test_prediction, test_labels, test_accuracy = inference(test_data_type1, gd_weights)\n",
    "print \"test accuracy \", test_accuracy, \" training loss: \", train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 : Training loss:  0.725195477227  Training Accuracy:  0.1444  Test Accuracy:  0.1453\n",
      "epoch  1 : Training loss:  0.731428871801  Training Accuracy:  0.2235  Test Accuracy:  0.2256\n",
      "epoch  2 : Training loss:  0.505359535526  Training Accuracy:  0.3745  Test Accuracy:  0.3866\n",
      "epoch  3 : Training loss:  0.329317922611  Training Accuracy:  0.6728  Test Accuracy:  0.6655\n",
      "epoch  4 : Training loss:  0.249179726576  Training Accuracy:  0.7373  Test Accuracy:  0.7335\n",
      "epoch  5 : Training loss:  0.219714681822  Training Accuracy:  0.7971  Test Accuracy:  0.7925\n",
      "epoch  6 : Training loss:  0.207415367117  Training Accuracy:  0.8072  Test Accuracy:  0.8052\n",
      "epoch  7 : Training loss:  0.200127428926  Training Accuracy:  0.8154  Test Accuracy:  0.8111\n",
      "epoch  8 : Training loss:  0.194126886143  Training Accuracy:  0.8202  Test Accuracy:  0.8197\n",
      "epoch  9 : Training loss:  0.189038139602  Training Accuracy:  0.8259  Test Accuracy:  0.8228\n",
      "epoch  10 : Training loss:  0.184727270167  Training Accuracy:  0.8285  Test Accuracy:  0.8274\n",
      "epoch  11 : Training loss:  0.180983394255  Training Accuracy:  0.833  Test Accuracy:  0.8314\n",
      "epoch  12 : Training loss:  0.177746218849  Training Accuracy:  0.8332  Test Accuracy:  0.8343\n",
      "epoch  13 : Training loss:  0.17489643436  Training Accuracy:  0.8357  Test Accuracy:  0.8365\n",
      "epoch  14 : Training loss:  0.172396803081  Training Accuracy:  0.8383  Test Accuracy:  0.8383\n",
      "epoch  15 : Training loss:  0.170176073823  Training Accuracy:  0.8394  Test Accuracy:  0.8394\n",
      "epoch  16 : Training loss:  0.168208223504  Training Accuracy:  0.8415  Test Accuracy:  0.8412\n",
      "epoch  17 : Training loss:  0.166448423824  Training Accuracy:  0.8432  Test Accuracy:  0.8428\n",
      "epoch  18 : Training loss:  0.164877303692  Training Accuracy:  0.8445  Test Accuracy:  0.8438\n",
      "epoch  19 : Training loss:  0.163465666724  Training Accuracy:  0.8466  Test Accuracy:  0.8445\n",
      "epoch  20 : Training loss:  0.162198548515  Training Accuracy:  0.8477  Test Accuracy:  0.846\n",
      "epoch  21 : Training loss:  0.161056376356  Training Accuracy:  0.8493  Test Accuracy:  0.8473\n",
      "epoch  22 : Training loss:  0.160027380136  Training Accuracy:  0.8511  Test Accuracy:  0.849\n",
      "epoch  23 : Training loss:  0.159098023204  Training Accuracy:  0.8516  Test Accuracy:  0.8494\n",
      "epoch  24 : Training loss:  0.158258993096  Training Accuracy:  0.8528  Test Accuracy:  0.8505\n",
      "epoch  25 : Training loss:  0.1575006053  Training Accuracy:  0.8537  Test Accuracy:  0.8512\n",
      "epoch  26 : Training loss:  0.156815482356  Training Accuracy:  0.8542  Test Accuracy:  0.8521\n",
      "epoch  27 : Training loss:  0.156196460914  Training Accuracy:  0.855  Test Accuracy:  0.8531\n",
      "epoch  28 : Training loss:  0.155637687098  Training Accuracy:  0.8563  Test Accuracy:  0.854\n",
      "epoch  29 : Training loss:  0.155133703616  Training Accuracy:  0.8565  Test Accuracy:  0.8554\n",
      "epoch  30 : Training loss:  0.15467984706  Training Accuracy:  0.8574  Test Accuracy:  0.8558\n",
      "epoch  31 : Training loss:  0.154271857976  Training Accuracy:  0.8579  Test Accuracy:  0.8572\n",
      "epoch  32 : Training loss:  0.153905997207  Training Accuracy:  0.859  Test Accuracy:  0.8577\n",
      "epoch  33 : Training loss:  0.153578876085  Training Accuracy:  0.8595  Test Accuracy:  0.8586\n",
      "epoch  34 : Training loss:  0.153287471823  Training Accuracy:  0.8601  Test Accuracy:  0.8589\n",
      "epoch  35 : Training loss:  0.153029047559  Training Accuracy:  0.8605  Test Accuracy:  0.8596\n",
      "epoch  36 : Training loss:  0.152801137539  Training Accuracy:  0.8612  Test Accuracy:  0.8605\n",
      "epoch  37 : Training loss:  0.15260150414  Training Accuracy:  0.8623  Test Accuracy:  0.8611\n",
      "epoch  38 : Training loss:  0.152428117693  Training Accuracy:  0.8627  Test Accuracy:  0.8613\n",
      "epoch  39 : Training loss:  0.152279129822  Training Accuracy:  0.8632  Test Accuracy:  0.8617\n",
      "epoch  40 : Training loss:  0.152152855276  Training Accuracy:  0.8639  Test Accuracy:  0.8623\n",
      "epoch  41 : Training loss:  0.152047753483  Training Accuracy:  0.8648  Test Accuracy:  0.8628\n",
      "epoch  42 : Training loss:  0.151962413815  Training Accuracy:  0.8655  Test Accuracy:  0.8633\n",
      "epoch  43 : Training loss:  0.151895541972  Training Accuracy:  0.8662  Test Accuracy:  0.864\n",
      "epoch  44 : Training loss:  0.151845948383  Training Accuracy:  0.8674  Test Accuracy:  0.8645\n",
      "epoch  45 : Training loss:  0.151812537802  Training Accuracy:  0.868  Test Accuracy:  0.8654\n",
      "epoch  46 : Training loss:  0.151794300207  Training Accuracy:  0.8689  Test Accuracy:  0.8662\n",
      "epoch  47 : Training loss:  0.151790302703  Training Accuracy:  0.8687  Test Accuracy:  0.8669\n",
      "epoch  48 : Training loss:  0.151799682342  Training Accuracy:  0.8693  Test Accuracy:  0.8675\n",
      "epoch  49 : Training loss:  0.151821639736  Training Accuracy:  0.8702  Test Accuracy:  0.8681\n",
      "epoch  50 : Training loss:  0.151855433353  Training Accuracy:  0.8707  Test Accuracy:  0.8686\n",
      "epoch  51 : Training loss:  0.151900374424  Training Accuracy:  0.8716  Test Accuracy:  0.869\n",
      "epoch  52 : Training loss:  0.151955822378  Training Accuracy:  0.8722  Test Accuracy:  0.8692\n",
      "epoch  53 : Training loss:  0.152021180741  Training Accuracy:  0.8724  Test Accuracy:  0.8693\n",
      "epoch  54 : Training loss:  0.152095893454  Training Accuracy:  0.8725  Test Accuracy:  0.8698\n",
      "epoch  55 : Training loss:  0.15217944155  Training Accuracy:  0.8725  Test Accuracy:  0.8704\n",
      "epoch  56 : Training loss:  0.152271340153  Training Accuracy:  0.873  Test Accuracy:  0.8706\n",
      "epoch  57 : Training loss:  0.152371135769  Training Accuracy:  0.8734  Test Accuracy:  0.8708\n",
      "epoch  58 : Training loss:  0.152478403827  Training Accuracy:  0.8738  Test Accuracy:  0.8713\n",
      "epoch  59 : Training loss:  0.152592746452  Training Accuracy:  0.874  Test Accuracy:  0.8716\n",
      "epoch  60 : Training loss:  0.152713790435  Training Accuracy:  0.8744  Test Accuracy:  0.872\n",
      "epoch  61 : Training loss:  0.15284118539  Training Accuracy:  0.8744  Test Accuracy:  0.8724\n",
      "epoch  62 : Training loss:  0.152974602074  Training Accuracy:  0.8746  Test Accuracy:  0.8724\n",
      "epoch  63 : Training loss:  0.153113730848  Training Accuracy:  0.875  Test Accuracy:  0.8729\n",
      "epoch  64 : Training loss:  0.153258280276  Training Accuracy:  0.8754  Test Accuracy:  0.8732\n",
      "epoch  65 : Training loss:  0.153407975843  Training Accuracy:  0.8756  Test Accuracy:  0.8732\n",
      "epoch  66 : Training loss:  0.153562558768  Training Accuracy:  0.876  Test Accuracy:  0.8739\n",
      "epoch  67 : Training loss:  0.153721784933  Training Accuracy:  0.8764  Test Accuracy:  0.8738\n",
      "epoch  68 : Training loss:  0.153885423883  Training Accuracy:  0.877  Test Accuracy:  0.8741\n",
      "epoch  69 : Training loss:  0.154053257915  Training Accuracy:  0.8776  Test Accuracy:  0.8743\n",
      "epoch  70 : Training loss:  0.154225081235  Training Accuracy:  0.8778  Test Accuracy:  0.8746\n",
      "epoch  71 : Training loss:  0.154400699179  Training Accuracy:  0.8782  Test Accuracy:  0.8748\n",
      "epoch  72 : Training loss:  0.154579927501  Training Accuracy:  0.8786  Test Accuracy:  0.8749\n",
      "epoch  73 : Training loss:  0.154762591708  Training Accuracy:  0.8788  Test Accuracy:  0.8754\n",
      "epoch  74 : Training loss:  0.154948526446  Training Accuracy:  0.879  Test Accuracy:  0.8756\n",
      "epoch  75 : Training loss:  0.155137574933  Training Accuracy:  0.879  Test Accuracy:  0.8758\n",
      "epoch  76 : Training loss:  0.155329588434  Training Accuracy:  0.8791  Test Accuracy:  0.8759\n",
      "epoch  77 : Training loss:  0.155524425773  Training Accuracy:  0.8791  Test Accuracy:  0.8761\n",
      "epoch  78 : Training loss:  0.155721952877  Training Accuracy:  0.8793  Test Accuracy:  0.8763\n",
      "epoch  79 : Training loss:  0.155922042356  Training Accuracy:  0.8794  Test Accuracy:  0.8762\n",
      "epoch  80 : Training loss:  0.156124573113  Training Accuracy:  0.8797  Test Accuracy:  0.8765\n",
      "epoch  81 : Training loss:  0.156329429975  Training Accuracy:  0.8797  Test Accuracy:  0.8764\n",
      "epoch  82 : Training loss:  0.156536503355  Training Accuracy:  0.8798  Test Accuracy:  0.8769\n",
      "epoch  83 : Training loss:  0.156745688934  Training Accuracy:  0.8798  Test Accuracy:  0.8774\n",
      "epoch  84 : Training loss:  0.156956887366  Training Accuracy:  0.8799  Test Accuracy:  0.8775\n",
      "epoch  85 : Training loss:  0.157170003999  Training Accuracy:  0.8799  Test Accuracy:  0.8779\n",
      "epoch  86 : Training loss:  0.157384948616  Training Accuracy:  0.8799  Test Accuracy:  0.8781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  87 : Training loss:  0.157601635198  Training Accuracy:  0.8803  Test Accuracy:  0.8783\n",
      "epoch  88 : Training loss:  0.157819981689  Training Accuracy:  0.8806  Test Accuracy:  0.8786\n",
      "epoch  89 : Training loss:  0.158039909792  Training Accuracy:  0.8808  Test Accuracy:  0.8785\n",
      "epoch  90 : Training loss:  0.158261344763  Training Accuracy:  0.8808  Test Accuracy:  0.8787\n",
      "epoch  91 : Training loss:  0.15848421523  Training Accuracy:  0.8813  Test Accuracy:  0.8788\n",
      "epoch  92 : Training loss:  0.158708453014  Training Accuracy:  0.8816  Test Accuracy:  0.879\n",
      "epoch  93 : Training loss:  0.158933992964  Training Accuracy:  0.8818  Test Accuracy:  0.8793\n",
      "epoch  94 : Training loss:  0.159160772806  Training Accuracy:  0.882  Test Accuracy:  0.8794\n",
      "epoch  95 : Training loss:  0.159388732991  Training Accuracy:  0.8822  Test Accuracy:  0.8795\n",
      "epoch  96 : Training loss:  0.159617816566  Training Accuracy:  0.8822  Test Accuracy:  0.8796\n",
      "epoch  97 : Training loss:  0.159847969038  Training Accuracy:  0.8823  Test Accuracy:  0.8801\n",
      "epoch  98 : Training loss:  0.160079138255  Training Accuracy:  0.8825  Test Accuracy:  0.8802\n",
      "epoch  99 : Training loss:  0.160311274294  Training Accuracy:  0.8825  Test Accuracy:  0.8805\n",
      "epoch  100 : Training loss:  0.160544329347  Training Accuracy:  0.8825  Test Accuracy:  0.8806\n",
      "epoch  101 : Training loss:  0.160778257624  Training Accuracy:  0.8825  Test Accuracy:  0.8809\n",
      "epoch  102 : Training loss:  0.161013015256  Training Accuracy:  0.8826  Test Accuracy:  0.881\n",
      "epoch  103 : Training loss:  0.161248560201  Training Accuracy:  0.8827  Test Accuracy:  0.8815\n",
      "epoch  104 : Training loss:  0.16148485216  Training Accuracy:  0.8829  Test Accuracy:  0.8816\n",
      "epoch  105 : Training loss:  0.161721852497  Training Accuracy:  0.8832  Test Accuracy:  0.8818\n",
      "epoch  106 : Training loss:  0.16195952416  Training Accuracy:  0.8836  Test Accuracy:  0.8819\n",
      "epoch  107 : Training loss:  0.162197831607  Training Accuracy:  0.8837  Test Accuracy:  0.8821\n",
      "epoch  108 : Training loss:  0.162436740741  Training Accuracy:  0.884  Test Accuracy:  0.8825\n",
      "epoch  109 : Training loss:  0.162676218842  Training Accuracy:  0.8839  Test Accuracy:  0.8827\n",
      "epoch  110 : Training loss:  0.162916234504  Training Accuracy:  0.8839  Test Accuracy:  0.8828\n",
      "epoch  111 : Training loss:  0.163156757578  Training Accuracy:  0.8842  Test Accuracy:  0.8832\n",
      "epoch  112 : Training loss:  0.163397759116  Training Accuracy:  0.8844  Test Accuracy:  0.8837\n",
      "epoch  113 : Training loss:  0.163639211319  Training Accuracy:  0.8844  Test Accuracy:  0.8837\n",
      "epoch  114 : Training loss:  0.163881087484  Training Accuracy:  0.8844  Test Accuracy:  0.8836\n",
      "epoch  115 : Training loss:  0.16412336196  Training Accuracy:  0.8846  Test Accuracy:  0.8837\n",
      "epoch  116 : Training loss:  0.164366010097  Training Accuracy:  0.8849  Test Accuracy:  0.8838\n",
      "epoch  117 : Training loss:  0.164609008211  Training Accuracy:  0.8848  Test Accuracy:  0.884\n",
      "epoch  118 : Training loss:  0.164852333536  Training Accuracy:  0.8849  Test Accuracy:  0.884\n",
      "epoch  119 : Training loss:  0.165095964188  Training Accuracy:  0.8849  Test Accuracy:  0.8841\n",
      "epoch  120 : Training loss:  0.165339879127  Training Accuracy:  0.8852  Test Accuracy:  0.884\n",
      "epoch  121 : Training loss:  0.165584058124  Training Accuracy:  0.8852  Test Accuracy:  0.884\n",
      "epoch  122 : Training loss:  0.165828481723  Training Accuracy:  0.8853  Test Accuracy:  0.884\n",
      "epoch  123 : Training loss:  0.166073131214  Training Accuracy:  0.8855  Test Accuracy:  0.8841\n",
      "epoch  124 : Training loss:  0.166317988597  Training Accuracy:  0.8858  Test Accuracy:  0.8844\n",
      "epoch  125 : Training loss:  0.166563036559  Training Accuracy:  0.886  Test Accuracy:  0.8846\n",
      "epoch  126 : Training loss:  0.166808258441  Training Accuracy:  0.886  Test Accuracy:  0.8847\n",
      "epoch  127 : Training loss:  0.167053638213  Training Accuracy:  0.8859  Test Accuracy:  0.8849\n",
      "epoch  128 : Training loss:  0.167299160452  Training Accuracy:  0.886  Test Accuracy:  0.8848\n",
      "epoch  129 : Training loss:  0.167544810313  Training Accuracy:  0.8861  Test Accuracy:  0.885\n",
      "epoch  130 : Training loss:  0.167790573508  Training Accuracy:  0.8862  Test Accuracy:  0.885\n",
      "epoch  131 : Training loss:  0.168036436284  Training Accuracy:  0.8863  Test Accuracy:  0.8851\n",
      "epoch  132 : Training loss:  0.168282385405  Training Accuracy:  0.8863  Test Accuracy:  0.8854\n",
      "epoch  133 : Training loss:  0.168528408125  Training Accuracy:  0.8868  Test Accuracy:  0.8852\n",
      "epoch  134 : Training loss:  0.168774492176  Training Accuracy:  0.887  Test Accuracy:  0.8855\n",
      "epoch  135 : Training loss:  0.169020625744  Training Accuracy:  0.887  Test Accuracy:  0.8858\n",
      "epoch  136 : Training loss:  0.169266797454  Training Accuracy:  0.8874  Test Accuracy:  0.8859\n",
      "epoch  137 : Training loss:  0.169512996356  Training Accuracy:  0.8874  Test Accuracy:  0.886\n",
      "epoch  138 : Training loss:  0.169759211903  Training Accuracy:  0.8874  Test Accuracy:  0.886\n",
      "epoch  139 : Training loss:  0.170005433939  Training Accuracy:  0.8875  Test Accuracy:  0.886\n",
      "epoch  140 : Training loss:  0.170251652685  Training Accuracy:  0.8876  Test Accuracy:  0.8862\n",
      "epoch  141 : Training loss:  0.170497858722  Training Accuracy:  0.8881  Test Accuracy:  0.8864\n",
      "epoch  142 : Training loss:  0.170744042981  Training Accuracy:  0.8881  Test Accuracy:  0.8865\n",
      "epoch  143 : Training loss:  0.170990196726  Training Accuracy:  0.8881  Test Accuracy:  0.8867\n",
      "epoch  144 : Training loss:  0.171236311547  Training Accuracy:  0.8881  Test Accuracy:  0.8868\n",
      "epoch  145 : Training loss:  0.171482379342  Training Accuracy:  0.8883  Test Accuracy:  0.8868\n",
      "epoch  146 : Training loss:  0.171728392309  Training Accuracy:  0.8884  Test Accuracy:  0.8868\n",
      "epoch  147 : Training loss:  0.171974342935  Training Accuracy:  0.8885  Test Accuracy:  0.8869\n",
      "epoch  148 : Training loss:  0.172220223985  Training Accuracy:  0.8885  Test Accuracy:  0.887\n",
      "epoch  149 : Training loss:  0.172466028492  Training Accuracy:  0.8887  Test Accuracy:  0.8872\n",
      "epoch  150 : Training loss:  0.172711749746  Training Accuracy:  0.8889  Test Accuracy:  0.8874\n",
      "epoch  151 : Training loss:  0.172957381288  Training Accuracy:  0.8892  Test Accuracy:  0.8875\n",
      "epoch  152 : Training loss:  0.173202916896  Training Accuracy:  0.8893  Test Accuracy:  0.8875\n",
      "epoch  153 : Training loss:  0.173448350583  Training Accuracy:  0.8893  Test Accuracy:  0.8877\n",
      "epoch  154 : Training loss:  0.173693676581  Training Accuracy:  0.8894  Test Accuracy:  0.8878\n",
      "epoch  155 : Training loss:  0.173938889341  Training Accuracy:  0.8896  Test Accuracy:  0.8879\n",
      "epoch  156 : Training loss:  0.17418398352  Training Accuracy:  0.8897  Test Accuracy:  0.8877\n",
      "epoch  157 : Training loss:  0.174428953975  Training Accuracy:  0.8897  Test Accuracy:  0.8878\n",
      "epoch  158 : Training loss:  0.174673795757  Training Accuracy:  0.8897  Test Accuracy:  0.8879\n",
      "epoch  159 : Training loss:  0.174918504103  Training Accuracy:  0.8897  Test Accuracy:  0.8882\n",
      "epoch  160 : Training loss:  0.17516307443  Training Accuracy:  0.8897  Test Accuracy:  0.8882\n",
      "epoch  161 : Training loss:  0.17540750233  Training Accuracy:  0.8897  Test Accuracy:  0.8883\n",
      "epoch  162 : Training loss:  0.17565178356  Training Accuracy:  0.8899  Test Accuracy:  0.8882\n",
      "epoch  163 : Training loss:  0.175895914042  Training Accuracy:  0.89  Test Accuracy:  0.8884\n",
      "epoch  164 : Training loss:  0.176139889853  Training Accuracy:  0.8899  Test Accuracy:  0.8885\n",
      "epoch  165 : Training loss:  0.176383707221  Training Accuracy:  0.8902  Test Accuracy:  0.8883\n",
      "epoch  166 : Training loss:  0.17662736252  Training Accuracy:  0.8903  Test Accuracy:  0.8882\n",
      "epoch  167 : Training loss:  0.176870852264  Training Accuracy:  0.8903  Test Accuracy:  0.8882\n",
      "epoch  168 : Training loss:  0.177114173106  Training Accuracy:  0.8903  Test Accuracy:  0.8884\n",
      "epoch  169 : Training loss:  0.177357321826  Training Accuracy:  0.8903  Test Accuracy:  0.8884\n",
      "epoch  170 : Training loss:  0.177600295335  Training Accuracy:  0.8904  Test Accuracy:  0.8884\n",
      "epoch  171 : Training loss:  0.177843090665  Training Accuracy:  0.8905  Test Accuracy:  0.8886\n",
      "epoch  172 : Training loss:  0.178085704967  Training Accuracy:  0.8905  Test Accuracy:  0.8887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  173 : Training loss:  0.178328135505  Training Accuracy:  0.8906  Test Accuracy:  0.8888\n",
      "epoch  174 : Training loss:  0.178570379657  Training Accuracy:  0.8905  Test Accuracy:  0.8888\n",
      "epoch  175 : Training loss:  0.178812434904  Training Accuracy:  0.8905  Test Accuracy:  0.8888\n",
      "epoch  176 : Training loss:  0.179054298836  Training Accuracy:  0.8906  Test Accuracy:  0.8888\n",
      "epoch  177 : Training loss:  0.179295969137  Training Accuracy:  0.8905  Test Accuracy:  0.8888\n",
      "epoch  178 : Training loss:  0.179537443592  Training Accuracy:  0.8905  Test Accuracy:  0.889\n",
      "epoch  179 : Training loss:  0.179778720077  Training Accuracy:  0.8907  Test Accuracy:  0.889\n",
      "epoch  180 : Training loss:  0.180019796562  Training Accuracy:  0.8908  Test Accuracy:  0.8891\n",
      "epoch  181 : Training loss:  0.180260671099  Training Accuracy:  0.8907  Test Accuracy:  0.8892\n",
      "epoch  182 : Training loss:  0.18050134183  Training Accuracy:  0.8906  Test Accuracy:  0.8892\n",
      "epoch  183 : Training loss:  0.180741806976  Training Accuracy:  0.8907  Test Accuracy:  0.8892\n",
      "epoch  184 : Training loss:  0.180982064836  Training Accuracy:  0.8907  Test Accuracy:  0.8892\n",
      "epoch  185 : Training loss:  0.181222113789  Training Accuracy:  0.8908  Test Accuracy:  0.8894\n",
      "epoch  186 : Training loss:  0.181461952285  Training Accuracy:  0.8909  Test Accuracy:  0.8893\n",
      "epoch  187 : Training loss:  0.181701578848  Training Accuracy:  0.8912  Test Accuracy:  0.8892\n",
      "epoch  188 : Training loss:  0.181940992068  Training Accuracy:  0.8912  Test Accuracy:  0.8892\n",
      "epoch  189 : Training loss:  0.182180190606  Training Accuracy:  0.8913  Test Accuracy:  0.8892\n",
      "epoch  190 : Training loss:  0.182419173184  Training Accuracy:  0.8914  Test Accuracy:  0.8891\n",
      "epoch  191 : Training loss:  0.18265793859  Training Accuracy:  0.8917  Test Accuracy:  0.8892\n",
      "epoch  192 : Training loss:  0.182896485671  Training Accuracy:  0.8919  Test Accuracy:  0.8891\n",
      "epoch  193 : Training loss:  0.183134813333  Training Accuracy:  0.8921  Test Accuracy:  0.8894\n",
      "epoch  194 : Training loss:  0.183372920538  Training Accuracy:  0.8923  Test Accuracy:  0.8893\n",
      "epoch  195 : Training loss:  0.183610806304  Training Accuracy:  0.8924  Test Accuracy:  0.8893\n",
      "epoch  196 : Training loss:  0.183848469703  Training Accuracy:  0.8925  Test Accuracy:  0.8894\n",
      "epoch  197 : Training loss:  0.184085909856  Training Accuracy:  0.8927  Test Accuracy:  0.8895\n",
      "epoch  198 : Training loss:  0.184323125936  Training Accuracy:  0.8929  Test Accuracy:  0.8896\n",
      "epoch  199 : Training loss:  0.184560117164  Training Accuracy:  0.8932  Test Accuracy:  0.8897\n",
      "epoch  200 : Training loss:  0.184796882806  Training Accuracy:  0.8933  Test Accuracy:  0.8898\n",
      "epoch  201 : Training loss:  0.185033422175  Training Accuracy:  0.8934  Test Accuracy:  0.8898\n",
      "epoch  202 : Training loss:  0.185269734626  Training Accuracy:  0.8935  Test Accuracy:  0.8898\n",
      "epoch  203 : Training loss:  0.185505819558  Training Accuracy:  0.8935  Test Accuracy:  0.89\n",
      "epoch  204 : Training loss:  0.18574167641  Training Accuracy:  0.8938  Test Accuracy:  0.89\n",
      "epoch  205 : Training loss:  0.185977304661  Training Accuracy:  0.8938  Test Accuracy:  0.89\n",
      "epoch  206 : Training loss:  0.186212703828  Training Accuracy:  0.8938  Test Accuracy:  0.8901\n",
      "epoch  207 : Training loss:  0.186447873464  Training Accuracy:  0.8939  Test Accuracy:  0.8901\n",
      "epoch  208 : Training loss:  0.186682813161  Training Accuracy:  0.8939  Test Accuracy:  0.8904\n",
      "epoch  209 : Training loss:  0.186917522542  Training Accuracy:  0.8939  Test Accuracy:  0.8904\n",
      "epoch  210 : Training loss:  0.187152001266  Training Accuracy:  0.8939  Test Accuracy:  0.8905\n",
      "epoch  211 : Training loss:  0.187386249024  Training Accuracy:  0.8941  Test Accuracy:  0.8905\n",
      "epoch  212 : Training loss:  0.187620265538  Training Accuracy:  0.8941  Test Accuracy:  0.8904\n",
      "epoch  213 : Training loss:  0.187854050562  Training Accuracy:  0.8941  Test Accuracy:  0.8903\n",
      "epoch  214 : Training loss:  0.188087603876  Training Accuracy:  0.8941  Test Accuracy:  0.8905\n",
      "epoch  215 : Training loss:  0.188320925293  Training Accuracy:  0.894  Test Accuracy:  0.8905\n",
      "epoch  216 : Training loss:  0.18855401465  Training Accuracy:  0.8942  Test Accuracy:  0.8906\n",
      "epoch  217 : Training loss:  0.188786871812  Training Accuracy:  0.8944  Test Accuracy:  0.8906\n",
      "epoch  218 : Training loss:  0.189019496671  Training Accuracy:  0.8944  Test Accuracy:  0.8906\n",
      "epoch  219 : Training loss:  0.189251889142  Training Accuracy:  0.8946  Test Accuracy:  0.8906\n",
      "epoch  220 : Training loss:  0.189484049164  Training Accuracy:  0.8946  Test Accuracy:  0.8907\n",
      "epoch  221 : Training loss:  0.189715976703  Training Accuracy:  0.8948  Test Accuracy:  0.8908\n",
      "epoch  222 : Training loss:  0.189947671742  Training Accuracy:  0.895  Test Accuracy:  0.8909\n",
      "epoch  223 : Training loss:  0.190179134291  Training Accuracy:  0.895  Test Accuracy:  0.8912\n",
      "epoch  224 : Training loss:  0.190410364377  Training Accuracy:  0.895  Test Accuracy:  0.8912\n",
      "epoch  225 : Training loss:  0.190641362051  Training Accuracy:  0.8953  Test Accuracy:  0.8912\n",
      "epoch  226 : Training loss:  0.190872127382  Training Accuracy:  0.8954  Test Accuracy:  0.8912\n",
      "epoch  227 : Training loss:  0.191102660458  Training Accuracy:  0.8956  Test Accuracy:  0.8913\n",
      "epoch  228 : Training loss:  0.191332961385  Training Accuracy:  0.8957  Test Accuracy:  0.8913\n",
      "epoch  229 : Training loss:  0.191563030288  Training Accuracy:  0.8957  Test Accuracy:  0.8913\n",
      "epoch  230 : Training loss:  0.19179286731  Training Accuracy:  0.8958  Test Accuracy:  0.8913\n",
      "epoch  231 : Training loss:  0.192022472608  Training Accuracy:  0.8959  Test Accuracy:  0.8913\n",
      "epoch  232 : Training loss:  0.192251846358  Training Accuracy:  0.896  Test Accuracy:  0.8913\n",
      "epoch  233 : Training loss:  0.192480988749  Training Accuracy:  0.8961  Test Accuracy:  0.8914\n",
      "epoch  234 : Training loss:  0.192709899987  Training Accuracy:  0.8961  Test Accuracy:  0.8914\n",
      "epoch  235 : Training loss:  0.192938580292  Training Accuracy:  0.896  Test Accuracy:  0.8915\n",
      "epoch  236 : Training loss:  0.193167029898  Training Accuracy:  0.8961  Test Accuracy:  0.8917\n",
      "epoch  237 : Training loss:  0.193395249051  Training Accuracy:  0.8963  Test Accuracy:  0.8917\n",
      "epoch  238 : Training loss:  0.193623238013  Training Accuracy:  0.8962  Test Accuracy:  0.8918\n",
      "epoch  239 : Training loss:  0.193850997057  Training Accuracy:  0.8963  Test Accuracy:  0.8918\n",
      "epoch  240 : Training loss:  0.194078526468  Training Accuracy:  0.8963  Test Accuracy:  0.8919\n",
      "epoch  241 : Training loss:  0.194305826544  Training Accuracy:  0.8963  Test Accuracy:  0.8918\n",
      "epoch  242 : Training loss:  0.194532897592  Training Accuracy:  0.8965  Test Accuracy:  0.892\n",
      "epoch  243 : Training loss:  0.194759739932  Training Accuracy:  0.8966  Test Accuracy:  0.8923\n",
      "epoch  244 : Training loss:  0.194986353895  Training Accuracy:  0.8966  Test Accuracy:  0.8923\n",
      "epoch  245 : Training loss:  0.19521273982  Training Accuracy:  0.8969  Test Accuracy:  0.8924\n",
      "epoch  246 : Training loss:  0.195438898058  Training Accuracy:  0.8969  Test Accuracy:  0.8924\n",
      "epoch  247 : Training loss:  0.195664828968  Training Accuracy:  0.8968  Test Accuracy:  0.8925\n",
      "epoch  248 : Training loss:  0.195890532918  Training Accuracy:  0.8968  Test Accuracy:  0.8925\n",
      "epoch  249 : Training loss:  0.196116010286  Training Accuracy:  0.8969  Test Accuracy:  0.8926\n",
      "epoch  250 : Training loss:  0.196341261459  Training Accuracy:  0.8969  Test Accuracy:  0.8928\n",
      "epoch  251 : Training loss:  0.196566286829  Training Accuracy:  0.8969  Test Accuracy:  0.8928\n",
      "epoch  252 : Training loss:  0.196791086799  Training Accuracy:  0.8971  Test Accuracy:  0.8929\n",
      "epoch  253 : Training loss:  0.197015661778  Training Accuracy:  0.8971  Test Accuracy:  0.8929\n",
      "epoch  254 : Training loss:  0.197240012183  Training Accuracy:  0.8971  Test Accuracy:  0.8929\n",
      "epoch  255 : Training loss:  0.197464138438  Training Accuracy:  0.8973  Test Accuracy:  0.8929\n",
      "epoch  256 : Training loss:  0.197688040972  Training Accuracy:  0.8972  Test Accuracy:  0.893\n",
      "epoch  257 : Training loss:  0.197911720223  Training Accuracy:  0.8973  Test Accuracy:  0.8933\n",
      "epoch  258 : Training loss:  0.198135176633  Training Accuracy:  0.8973  Test Accuracy:  0.8936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  259 : Training loss:  0.198358410652  Training Accuracy:  0.8973  Test Accuracy:  0.8936\n",
      "epoch  260 : Training loss:  0.198581422734  Training Accuracy:  0.8973  Test Accuracy:  0.8937\n",
      "epoch  261 : Training loss:  0.198804213338  Training Accuracy:  0.8974  Test Accuracy:  0.8938\n",
      "epoch  262 : Training loss:  0.199026782931  Training Accuracy:  0.8975  Test Accuracy:  0.8937\n",
      "epoch  263 : Training loss:  0.199249131983  Training Accuracy:  0.8974  Test Accuracy:  0.8938\n",
      "epoch  264 : Training loss:  0.199471260968  Training Accuracy:  0.8975  Test Accuracy:  0.8938\n",
      "epoch  265 : Training loss:  0.199693170366  Training Accuracy:  0.8975  Test Accuracy:  0.8938\n",
      "epoch  266 : Training loss:  0.199914860661  Training Accuracy:  0.8977  Test Accuracy:  0.8939\n",
      "epoch  267 : Training loss:  0.200136332342  Training Accuracy:  0.8978  Test Accuracy:  0.8939\n",
      "epoch  268 : Training loss:  0.200357585901  Training Accuracy:  0.8977  Test Accuracy:  0.8939\n",
      "epoch  269 : Training loss:  0.200578621834  Training Accuracy:  0.8978  Test Accuracy:  0.8938\n",
      "epoch  270 : Training loss:  0.20079944064  Training Accuracy:  0.8978  Test Accuracy:  0.8938\n",
      "epoch  271 : Training loss:  0.201020042823  Training Accuracy:  0.8978  Test Accuracy:  0.8938\n",
      "epoch  272 : Training loss:  0.201240428889  Training Accuracy:  0.8978  Test Accuracy:  0.8939\n",
      "epoch  273 : Training loss:  0.201460599347  Training Accuracy:  0.898  Test Accuracy:  0.894\n",
      "epoch  274 : Training loss:  0.201680554712  Training Accuracy:  0.8981  Test Accuracy:  0.894\n",
      "epoch  275 : Training loss:  0.201900295497  Training Accuracy:  0.8982  Test Accuracy:  0.8941\n",
      "epoch  276 : Training loss:  0.20211982222  Training Accuracy:  0.8982  Test Accuracy:  0.8941\n",
      "epoch  277 : Training loss:  0.202339135404  Training Accuracy:  0.8982  Test Accuracy:  0.8942\n",
      "epoch  278 : Training loss:  0.202558235569  Training Accuracy:  0.8982  Test Accuracy:  0.8942\n",
      "epoch  279 : Training loss:  0.202777123243  Training Accuracy:  0.8984  Test Accuracy:  0.8942\n",
      "epoch  280 : Training loss:  0.202995798951  Training Accuracy:  0.8985  Test Accuracy:  0.8943\n",
      "epoch  281 : Training loss:  0.203214263224  Training Accuracy:  0.8986  Test Accuracy:  0.8944\n",
      "epoch  282 : Training loss:  0.203432516592  Training Accuracy:  0.8985  Test Accuracy:  0.8943\n",
      "epoch  283 : Training loss:  0.203650559589  Training Accuracy:  0.8985  Test Accuracy:  0.8943\n",
      "epoch  284 : Training loss:  0.203868392749  Training Accuracy:  0.8985  Test Accuracy:  0.8942\n",
      "epoch  285 : Training loss:  0.204086016608  Training Accuracy:  0.8986  Test Accuracy:  0.8942\n",
      "epoch  286 : Training loss:  0.204303431703  Training Accuracy:  0.8988  Test Accuracy:  0.8942\n",
      "epoch  287 : Training loss:  0.204520638574  Training Accuracy:  0.8986  Test Accuracy:  0.8943\n",
      "epoch  288 : Training loss:  0.204737637761  Training Accuracy:  0.8986  Test Accuracy:  0.8943\n",
      "epoch  289 : Training loss:  0.204954429804  Training Accuracy:  0.8987  Test Accuracy:  0.8942\n",
      "epoch  290 : Training loss:  0.205171015246  Training Accuracy:  0.8988  Test Accuracy:  0.8942\n",
      "epoch  291 : Training loss:  0.205387394629  Training Accuracy:  0.8989  Test Accuracy:  0.8941\n",
      "epoch  292 : Training loss:  0.205603568497  Training Accuracy:  0.8989  Test Accuracy:  0.8942\n",
      "epoch  293 : Training loss:  0.205819537395  Training Accuracy:  0.899  Test Accuracy:  0.8942\n",
      "epoch  294 : Training loss:  0.206035301867  Training Accuracy:  0.8991  Test Accuracy:  0.8943\n",
      "epoch  295 : Training loss:  0.20625086246  Training Accuracy:  0.8991  Test Accuracy:  0.8943\n",
      "epoch  296 : Training loss:  0.20646621972  Training Accuracy:  0.8991  Test Accuracy:  0.8942\n",
      "epoch  297 : Training loss:  0.206681374192  Training Accuracy:  0.8993  Test Accuracy:  0.8942\n",
      "epoch  298 : Training loss:  0.206896326425  Training Accuracy:  0.8993  Test Accuracy:  0.8944\n",
      "epoch  299 : Training loss:  0.207111076964  Training Accuracy:  0.8993  Test Accuracy:  0.8945\n",
      "test accuracy  0.8945  training loss:  0.207111076964\n"
     ]
    }
   ],
   "source": [
    "gd_weights, train_loss = train_gd(train_data_type2[:10000], epochs, learning_rate, lamda, test_data_type2)\n",
    "test_prediction, test_labels, test_accuracy = inference(test_data_type2, gd_weights)\n",
    "print \"test accuracy \", test_accuracy, \" training loss: \", train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.zeros((10,1))\n",
    "sigmoid(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
